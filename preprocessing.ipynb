import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
     

dataset = pd.read_csv('datakelulusanmahasiswa.csv')
x = dataset.iloc[:, :-1].values
y = dataset.iloc[:, -1].values
     

print(x)
     
[['ANIK WIDAYANTI' 28 2.76 2.8 3.2 3.17 2.98 3.0 3.03 nan 3.07]
 ['DWI HESTYNA PRIHASTANTY' 32 3.0 3.3 3.14 3.14 2.84 3.13 3.25 nan 3.17]
 ['MURYA ARIEF BASUKI' 29 3.5 3.3 3.7 3.29 3.53 3.72 3.73 nan 3.54]
 ['NANIK SUSANTI' 27 3.17 3.41 3.61 3.36 3.48 3.63 3.46 nan 3.41]
 ['RIFKA ISTIQFARINA' 29 2.9 2.89 3.3 2.85 2.98 3.0 3.08 nan 3.09]
 ['SUHARYONO' 27 2.95 2.82 3.09 3.1 2.78 3.16 3.23 nan 3.07]
 ['FARIKHATUN NAZLI' 26 2.76 3.14 2.6 2.95 3.23 3.33 3.3 3.3 3.06]
 ['FIFI SUNALISA' 27 2.62 2.89 2.32 2.5 2.5 2.86 3.05 2.5 2.91]
 ['HENDRIK MULIYANTO' 25 3.6 3.54 3.52 3.39 3.52 3.68 3.15 nan 3.4]
 ['IMAM AGUNG RIBOWO' 28 2.71 2.55 1.77 2.11 1.93 2.13 1.78 0.2 2.2]
 ['IMAM SANTOSA' 27 3.14 3.46 3.4 3.43 3.27 3.15 3.81 4.0 3.44]
 ['IRFAN EKO WAHYUDI' 32 2.67 2.3 1.57 1.44 1.58 1.68 1.13 0.94 2.4]
 ['IWAN HAMBALI' 26 2.57 2.82 2.2 2.45 2.1 2.42 1.0 1.42 2.45]
 ['M SYAIFULLAH' 31 2.71 3.0 2.65 2.27 2.13 3.34 2.5 nan 2.57]
 ['DIANA LAILY FITHRI' 26 3.24 3.38 3.44 3.3 3.56 3.45 3.35 3.35 3.45]
 ['DONNY PRABOWO' 27 2.86 2.86 2.45 1.86 3.19 3.14 2.98 2.0 3.04]
 ['EDI JATMIKO' 27 2.71 3.27 2.54 3.36 3.23 3.28 3.15 2.0 3.19]
 ['ADI SUBEKTI' 29 2.67 2.2 1.45 3.0 3.17 3.08 3.33 nan 2.95]
 ['AHMAD IBROZI' 27 2.67 2.68 1.95 1.61 2.36 2.21 0.78 0.92 2.11]]

print(y)
     
['TERLAMBAT' 'TERLAMBAT' 'TERLAMBAT' 'TERLAMBAT' 'TERLAMBAT' 'TERLAMBAT'
 'TEPAT' 'TEPAT' 'TERLAMBAT' 'TERLAMBAT' 'TERLAMBAT' 'TERLAMBAT'
 'TERLAMBAT' 'TERLAMBAT' 'TERLAMBAT' 'TERLAMBAT' 'TERLAMBAT' 'TERLAMBAT'
 'TERLAMBAT']

from sklearn.impute import SimpleImputer
imputer = SimpleImputer(missing_values=np.nan, strategy='mean')
imputer.fit(x[:, 1:11])
x[:, 1:11] = imputer.transform(x[:, 1:11])
     

print(x)
     
[['ANIK WIDAYANTI' 28.0 2.76 2.8 3.2 3.17 2.98 3.0 3.03 2.063 3.07]
 ['DWI HESTYNA PRIHASTANTY' 32.0 3.0 3.3 3.14 3.14 2.84 3.13 3.25 2.063
  3.17]
 ['MURYA ARIEF BASUKI' 29.0 3.5 3.3 3.7 3.29 3.53 3.72 3.73 2.063 3.54]
 ['NANIK SUSANTI' 27.0 3.17 3.41 3.61 3.36 3.48 3.63 3.46 2.063 3.41]
 ['RIFKA ISTIQFARINA' 29.0 2.9 2.89 3.3 2.85 2.98 3.0 3.08 2.063 3.09]
 ['SUHARYONO' 27.0 2.95 2.82 3.09 3.1 2.78 3.16 3.23 2.063 3.07]
 ['FARIKHATUN NAZLI' 26.0 2.76 3.14 2.6 2.95 3.23 3.33 3.3 3.3 3.06]
 ['FIFI SUNALISA' 27.0 2.62 2.89 2.32 2.5 2.5 2.86 3.05 2.5 2.91]
 ['HENDRIK MULIYANTO' 25.0 3.6 3.54 3.52 3.39 3.52 3.68 3.15 2.063 3.4]
 ['IMAM AGUNG RIBOWO' 28.0 2.71 2.55 1.77 2.11 1.93 2.13 1.78 0.2 2.2]
 ['IMAM SANTOSA' 27.0 3.14 3.46 3.4 3.43 3.27 3.15 3.81 4.0 3.44]
 ['IRFAN EKO WAHYUDI' 32.0 2.67 2.3 1.57 1.44 1.58 1.68 1.13 0.94 2.4]
 ['IWAN HAMBALI' 26.0 2.57 2.82 2.2 2.45 2.1 2.42 1.0 1.42 2.45]
 ['M SYAIFULLAH' 31.0 2.71 3.0 2.65 2.27 2.13 3.34 2.5 2.063 2.57]
 ['DIANA LAILY FITHRI' 26.0 3.24 3.38 3.44 3.3 3.56 3.45 3.35 3.35 3.45]
 ['DONNY PRABOWO' 27.0 2.86 2.86 2.45 1.86 3.19 3.14 2.98 2.0 3.04]
 ['EDI JATMIKO' 27.0 2.71 3.27 2.54 3.36 3.23 3.28 3.15 2.0 3.19]
 ['ADI SUBEKTI' 29.0 2.67 2.2 1.45 3.0 3.17 3.08 3.33 2.063 2.95]
 ['AHMAD IBROZI' 27.0 2.67 2.68 1.95 1.61 2.36 2.21 0.78 0.92 2.11]]

from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder
ct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), [0])], remainder='passthrough')
x = np.array(ct.fit_transform(x))
     

print(x)
     
[[0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 28.0 2.76 2.8 3.2 3.17 2.98 3.0 3.03 2.063 3.07]
 [0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 32.0 3.0 3.3 3.14 3.14 2.84 3.13 3.25 2.063 3.17]
 [0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0
  0.0 29.0 3.5 3.3 3.7 3.29 3.53 3.72 3.73 2.063 3.54]
 [0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0
  0.0 27.0 3.17 3.41 3.61 3.36 3.48 3.63 3.46 2.063 3.41]
 [0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0
  0.0 29.0 2.9 2.89 3.3 2.85 2.98 3.0 3.08 2.063 3.09]
 [0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  1.0 27.0 2.95 2.82 3.09 3.1 2.78 3.16 3.23 2.063 3.07]
 [0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 26.0 2.76 3.14 2.6 2.95 3.23 3.33 3.3 3.3 3.06]
 [0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 27.0 2.62 2.89 2.32 2.5 2.5 2.86 3.05 2.5 2.91]
 [0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 25.0 3.6 3.54 3.52 3.39 3.52 3.68 3.15 2.063 3.4]
 [0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 28.0 2.71 2.55 1.77 2.11 1.93 2.13 1.78 0.2 2.2]
 [0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 27.0 3.14 3.46 3.4 3.43 3.27 3.15 3.81 4.0 3.44]
 [0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0
  0.0 32.0 2.67 2.3 1.57 1.44 1.58 1.68 1.13 0.94 2.4]
 [0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0
  0.0 26.0 2.57 2.82 2.2 2.45 2.1 2.42 1.0 1.42 2.45]
 [0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0
  0.0 31.0 2.71 3.0 2.65 2.27 2.13 3.34 2.5 2.063 2.57]
 [0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 26.0 3.24 3.38 3.44 3.3 3.56 3.45 3.35 3.35 3.45]
 [0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 27.0 2.86 2.86 2.45 1.86 3.19 3.14 2.98 2.0 3.04]
 [0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 27.0 2.71 3.27 2.54 3.36 3.23 3.28 3.15 2.0 3.19]
 [1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 29.0 2.67 2.2 1.45 3.0 3.17 3.08 3.33 2.063 2.95]
 [0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 27.0 2.67 2.68 1.95 1.61 2.36 2.21 0.78 0.92 2.11]]

from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
y = le.fit_transform(y)
     

print(y)
     
[1 1 1 1 1 1 0 0 1 1 1 1 1 1 1 1 1 1 1]

from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=1)
     

print(x_train)
     
[[0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0
  0.0 29.0 3.5 3.3 3.7 3.29 3.53 3.72 3.73 2.063 3.54]
 [0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 26.0 3.24 3.38 3.44 3.3 3.56 3.45 3.35 3.35 3.45]
 [0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0
  0.0 29.0 2.9 2.89 3.3 2.85 2.98 3.0 3.08 2.063 3.09]
 [0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 27.0 2.71 3.27 2.54 3.36 3.23 3.28 3.15 2.0 3.19]
 [0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 27.0 2.62 2.89 2.32 2.5 2.5 2.86 3.05 2.5 2.91]
 [0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 32.0 3.0 3.3 3.14 3.14 2.84 3.13 3.25 2.063 3.17]
 [0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0
  0.0 31.0 2.71 3.0 2.65 2.27 2.13 3.34 2.5 2.063 2.57]
 [0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 28.0 2.76 2.8 3.2 3.17 2.98 3.0 3.03 2.063 3.07]
 [0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 27.0 2.67 2.68 1.95 1.61 2.36 2.21 0.78 0.92 2.11]
 [1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 29.0 2.67 2.2 1.45 3.0 3.17 3.08 3.33 2.063 2.95]
 [0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 28.0 2.71 2.55 1.77 2.11 1.93 2.13 1.78 0.2 2.2]
 [0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 25.0 3.6 3.54 3.52 3.39 3.52 3.68 3.15 2.063 3.4]
 [0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0
  0.0 26.0 2.57 2.82 2.2 2.45 2.1 2.42 1.0 1.42 2.45]
 [0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0
  0.0 32.0 2.67 2.3 1.57 1.44 1.58 1.68 1.13 0.94 2.4]
 [0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  1.0 27.0 2.95 2.82 3.09 3.1 2.78 3.16 3.23 2.063 3.07]]

print(x_test)
     
[[0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0
  0.0 27.0 3.17 3.41 3.61 3.36 3.48 3.63 3.46 2.063 3.41]
 [0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 27.0 2.86 2.86 2.45 1.86 3.19 3.14 2.98 2.0 3.04]
 [0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 26.0 2.76 3.14 2.6 2.95 3.23 3.33 3.3 3.3 3.06]
 [0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 0.0 0.0 0.0 0.0
  0.0 27.0 3.14 3.46 3.4 3.43 3.27 3.15 3.81 4.0 3.44]]

print(y_train)
     
[1 1 1 1 0 1 1 1 1 1 1 1 1 1 1]

print(y_test)
     
[1 1 0 1]

from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
x_train[:, 3:] = sc.fit_transform(x_train[:, 3:])
x_test[:, 3:] = sc.fit_transform(x_test[:, 3:])
     

print(x_train)
     
[[0.0 0.0 0.0 -0.2672612419124243 0.0 -0.2672612419124243
  -0.2672612419124243 0.0 -0.2672612419124243 -0.2672612419124244
  -0.2672612419124244 0.0 -0.2672612419124244 -0.2672612419124244
  -0.26726124191242434 3.7416573867739404 0.0 -0.2672612419124243
  -0.2672612419124244 0.38609367125267247 1.9778122831124232
  1.014997277393135 1.4444417175875517 0.9049859784704201
  1.3016672590078404 1.3661565390059642 1.1701701563299312
  0.2902685997516879 1.44723897963748]
 [0.0 0.0 0.0 3.7416573867739404 0.0 -0.2672612419124243
  -0.2672612419124243 0.0 -0.2672612419124243 -0.2672612419124244
  -0.2672612419124244 0.0 -0.2672612419124244 -0.2672612419124244
  -0.26726124191242434 -0.2672612419124243 0.0 -0.2672612419124243
  -0.2672612419124244 -1.061757595944848 1.141210558151636
  1.226455043516705 1.0847148530542532 0.9212043651813591
  1.351475955143345 0.8916338989567228 0.7637125151915638
  2.09150142463318 1.2422261126374177]
 [0.0 0.0 0.0 -0.2672612419124243 0.0 -0.2672612419124243
  -0.2672612419124243 0.0 -0.2672612419124243 -0.2672612419124244
  -0.2672612419124244 0.0 -0.2672612419124244 -0.2672612419124244
  -0.26726124191242434 -0.2672612419124243 0.0 3.7416573867739404
  -0.2672612419124244 0.38609367125267247 0.047192917818297084
  -0.06872377399015843 0.8910157721517077 0.19137696318908567
  0.38850782985693155 0.10076283220798689 0.4749136649090397
  0.2902685997516879 0.4221746446371662]
 [0.0 0.0 0.0 -0.2672612419124243 0.0 -0.2672612419124243
  3.7416573867739404 0.0 -0.2672612419124243 -0.2672612419124244
  -0.2672612419124244 0.0 -0.2672612419124244 -0.2672612419124244
  -0.26726124191242434 -0.2672612419124243 0.0 -0.2672612419124243
  -0.2672612419124244 -0.5791405068790078 -0.5641698811915092
  0.935700615096797 -0.1604935241763948 1.0185146854469957
  0.8035802976527994 0.5928603848516443 0.5497874409082124
  0.20209636356867755 0.6499667190816806]
 [0.0 0.0 0.0 -0.2672612419124243 0.0 -0.2672612419124243
  -0.2672612419124243 0.0 3.7416573867739404 -0.2672612419124244
  -0.2672612419124244 0.0 -0.2672612419124244 -0.2672612419124244
  -0.26726124191242434 -0.2672612419124243 0.0 -0.2672612419124243
  -0.2672612419124244 -0.5791405068790078 -0.8537627859856277
  -0.06872377399015843 -0.464877794166109 -0.3762665716937941
  -0.40843130831113467 -0.14528594411384219 0.4428249037665367
  0.9018760158147895 0.012148910637041438]
 [0.0 0.0 0.0 -0.2672612419124243 0.0 3.7416573867739404
  -0.2672612419124243 0.0 -0.2672612419124243 -0.2672612419124244
  -0.2672612419124244 0.0 -0.2672612419124244 -0.2672612419124244
  -0.26726124191242434 -0.2672612419124243 0.0 -0.2672612419124243
  -0.2672612419124244 1.833944938450193 0.368962812033985
  1.014997277393135 0.6696453939773707 0.6617101778063289
  0.15606724789124535 0.3292366959353992 0.6567499780498881
  0.2902685997516879 0.6044083041927777]
 [0.0 0.0 0.0 -0.2672612419124243 0.0 -0.2672612419124243
  -0.2672612419124243 0.0 -0.2672612419124243 -0.2672612419124244
  -0.2672612419124244 0.0 -0.2672612419124244 -0.2672612419124244
  3.741657386773941 -0.2672612419124243 0.0 -0.2672612419124243
  -0.2672612419124244 1.3513278493843526 -0.5641698811915092
  0.22203065442974948 -0.008301389181537968 -0.7492894660454007
  -1.0227385606490191 0.6983098604181425 -0.14546905051267897
  0.2902685997516879 -0.7623441424743073]
 [0.0 0.0 1.0 -0.2672612419124243 0.0 -0.2672612419124243
  -0.2672612419124243 0.0 -0.2672612419124243 -0.2672612419124244
  -0.2672612419124244 0.0 -0.2672612419124244 -0.2672612419124244
  -0.26726124191242434 -0.2672612419124243 0.0 -0.2672612419124243
  -0.2672612419124244 -0.09652341781316769 -0.403284934083666
  -0.3066137608791751 0.7526592857927473 0.7103653379391468
  0.38850782985693155 0.10076283220798689 0.42143239633820156
  0.2902685997516879 0.37661622974826336]
 [0.0 1.0 0.0 -0.2672612419124243 0.0 -0.2672612419124243
  -0.2672612419124243 0.0 -0.2672612419124243 -0.2672612419124244
  -0.2672612419124244 0.0 -0.2672612419124244 -0.2672612419124244
  -0.26726124191242434 -0.2672612419124243 0.0 -0.2672612419124243
  -0.2672612419124244 -0.5791405068790078 -0.6928778388777844
  -0.6238004100645286 -0.9767967936942641 -1.8197029889674023
  -0.6408718902768209 -1.2876552627509046 -1.9852246893494996
  -1.3094276852829243 -1.8101876849190721]
 [1.0 0.0 0.0 -0.2672612419124243 0.0 -0.2672612419124243
  -0.2672612419124243 0.0 -0.2672612419124243 -0.2672612419124244
  -0.2672612419124244 0.0 -0.2672612419124244 -0.2672612419124244
  -0.26726124191242434 -0.2672612419124243 0.0 -0.2672612419124243
  -0.2672612419124244 0.38609367125267247 -0.6928778388777844
  -1.8925470068059465 -1.6685792254890686 0.4346527638531768
  0.703962905381791 0.2413621329633178 0.7423200077632287
  0.2902685997516879 0.10326574041484717]
 [0.0 0.0 0.0 -0.2672612419124243 0.0 -0.2672612419124243
  -0.2672612419124243 0.0 -0.2672612419124243 -0.2672612419124244
  3.7416573867739413 0.0 -0.2672612419124244 -0.2672612419124244
  -0.26726124191242434 -0.2672612419124243 0.0 -0.2672612419124243
  -0.2672612419124244 -0.09652341781316769 -0.5641698811915092
  -0.9674192800153302 -1.2258384691403936 -1.0087836534204317
  -1.3547965348857134 -1.4282545635062356 -0.9155993179327434
  -2.3171103845173255 -1.6051748179190088]
 [0.0 0.0 0.0 -0.2672612419124243 0.0 -0.2672612419124243
  -0.2672612419124243 0.0 -0.2672612419124243 3.7416573867739413
  -0.2672612419124244 0.0 -0.2672612419124244 -0.2672612419124244
  -0.26726124191242434 -0.2672612419124243 0.0 -0.2672612419124243
  -0.2672612419124244 -1.544374685010688 2.2995821773281113
  1.6493705757638446 1.195400042141422 1.0671698455798144
  1.2850643602960061 1.2958568886282988 0.5497874409082124
  0.2902685997516879 1.1283300754151602]
 [0.0 0.0 0.0 -0.2672612419124243 0.0 -0.2672612419124243
  -0.2672612419124243 0.0 -0.2672612419124243 -0.2672612419124244
  -0.2672612419124244 0.0 -0.2672612419124244 3.7416573867739413
  -0.26726124191242434 -0.2672612419124243 0.0 -0.2672612419124243
  -0.2672612419124244 -1.061757595944848 -1.0146477330934724
  -0.2537493193482826 -0.6309055777968616 -0.4573585052484909
  -1.0725472567845231 -0.9185820982681614 -1.7499071076378132
  -0.6096480330368124 -1.0356946318077234]
 [0.0 0.0 0.0 -0.2672612419124243 0.0 -0.2672612419124243
  -0.2672612419124243 0.0 -0.2672612419124243 -0.2672612419124244
  -0.2672612419124244 0.0 3.7416573867739413 -0.2672612419124244
  -0.26726124191242434 -0.2672612419124243 0.0 -0.2672612419124243
  -0.2672612419124244 1.833944938450193 -0.6928778388777844
  -1.6282247991514853 -1.5025514418583155 -2.095415563053373
  -1.9358979897999282 -2.2191256302549713 -1.610855809353635
  -1.2814364991930798 -1.149590669029981]
 [0.0 0.0 0.0 -0.2672612419124243 0.0 -0.2672612419124243
  -0.2672612419124243 0.0 -0.2672612419124243 -0.2672612419124244
  -0.2672612419124244 0.0 -0.2672612419124244 -0.2672612419124244
  -0.26726124191242434 -0.2672612419124243 0.0 -0.2672612419124243
  3.7416573867739413 -0.5791405068790078 0.20807786492614178
  -0.2537493193482826 0.6004671507978899 0.5968366309625711
  0.056449855620236966 0.3819614337186487 0.635357470621553
  0.2902685997516879 0.37661622974826336]]

print(x_test)
     
[[0.0 0.0 0.0 0.0 -0.5773502691896258 0.0 0.0 -0.5773502691896258 0.0 0.0
  0.0 -0.5773502691896258 0.0 0.0 0.0 0.0 1.7320508075688774 0.0 0.0
  0.5773502691896258 1.0628945990551184 0.8033482777887463
  1.1937663051642378 0.732701475699828 1.6757946081328192
  1.6011937209662281 0.24251866272383008 -0.9187387115537634
  0.9178811204042068]
 [0.0 0.0 0.0 0.0 1.7320508075688774 0.0 0.0 -0.5773502691896258 0.0 0.0
  0.0 -0.5773502691896258 0.0 0.0 0.0 0.0 -0.5773502691896258 0.0 0.0
  0.5773502691896258 -0.6944244713826777 -1.4919325158933883
  -1.1335764074248649 -1.656542466799611 -0.9161010524459414
  -0.8699398956430685 -1.3631221387580743 -0.9931592050643867
  -1.0509073697381468]
 [0.0 0.0 0.0 0.0 -0.5773502691896258 0.0 0.0 1.7320508075688774 0.0 0.0
  0.0 -0.5773502691896258 0.0 0.0 0.0 0.0 -0.5773502691896258 0.0 0.0
  -1.7320508075688774 -1.2613015908787413 -0.32342593001884656
  -0.8326269187279982 0.07964146474998174 -0.5585982027109397
  0.08825477202176099 -0.2926949377701385 0.5425017721389469
  -0.9444863702709925]
 [0.0 0.0 0.0 0.0 -0.5773502691896258 0.0 0.0 -0.5773502691896258 0.0 0.0
  0.0 1.7320508075688774 0.0 0.0 0.0 0.0 -0.5773502691896258 0.0 0.0
  0.5773502691896258 0.8928314632063006 1.012010168123485
  0.7724370209886243 0.8441995263498023 -0.20109535297593797
  -0.8195085973449205 1.4132984138043856 1.3693961444792038
  1.0775126196049372]]
